{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"\u6587\u6863\u642d\u5efa \u00b6 \u5b89\u88c5 \u00b6 pip install mkdocs \u521b\u5efa\u9879\u76ee \u00b6 mkdocs new . mkdocs.yml: \u914d\u7f6e\u6587\u4ef6 docs: \u6587\u6863\u6e90\u6587\u4ef6 \u542f\u52a8\u670d\u52a1 \u00b6 mkdocs serve \u6dfb\u52a0\u6587\u7ae0 \u00b6 \u521b\u5efa\u5bfc\u822a nav: - \u4e3b\u9875: 'index.md' - '\u4e91\u539f\u751f': - '\u57fa\u4e8ekubeadm\u5b89\u88c5kubernetes\u96c6\u7fa4': 'k8s/install_docker_k8s.md' \u5efa\u7acb\u7f51\u7ad9 \u00b6 mkdocs build \u90e8\u7f72\u6587\u6863 \u00b6 mkdocs gh-deploy","title":"\u4e3b\u9875"},{"location":"index.html#_1","text":"","title":"\u6587\u6863\u642d\u5efa"},{"location":"index.html#_2","text":"pip install mkdocs","title":"\u5b89\u88c5"},{"location":"index.html#_3","text":"mkdocs new . mkdocs.yml: \u914d\u7f6e\u6587\u4ef6 docs: \u6587\u6863\u6e90\u6587\u4ef6","title":"\u521b\u5efa\u9879\u76ee"},{"location":"index.html#_4","text":"mkdocs serve","title":"\u542f\u52a8\u670d\u52a1"},{"location":"index.html#_5","text":"\u521b\u5efa\u5bfc\u822a nav: - \u4e3b\u9875: 'index.md' - '\u4e91\u539f\u751f': - '\u57fa\u4e8ekubeadm\u5b89\u88c5kubernetes\u96c6\u7fa4': 'k8s/install_docker_k8s.md'","title":"\u6dfb\u52a0\u6587\u7ae0"},{"location":"index.html#_6","text":"mkdocs build","title":"\u5efa\u7acb\u7f51\u7ad9"},{"location":"index.html#_7","text":"mkdocs gh-deploy","title":"\u90e8\u7f72\u6587\u6863"},{"location":"cri/docker_proxy.html","text":"Docker\u4ee3\u7406 \u00b6 mkdir -p /etc/systemd/system/docker.service.d cat > /etc/systemd/system/docker.service.d/proxy.conf << EOF [Service] Environment=\"HTTP_PROXY=http://172.16.192.1:7890/\" Environment=\"HTTPS_PROXY=http://172.16.192.1:7890/\" Environment=\"NO_PROXY=localhost,127.0.0.1,.linux.io,172.17.0.0/16\" EOF systemctl daemon-reload && systemctl restart docker docker pull registry.k8s.io/pause:3.6","title":"\u914d\u7f6eDocker\u4ee3\u7406"},{"location":"cri/docker_proxy.html#docker","text":"mkdir -p /etc/systemd/system/docker.service.d cat > /etc/systemd/system/docker.service.d/proxy.conf << EOF [Service] Environment=\"HTTP_PROXY=http://172.16.192.1:7890/\" Environment=\"HTTPS_PROXY=http://172.16.192.1:7890/\" Environment=\"NO_PROXY=localhost,127.0.0.1,.linux.io,172.17.0.0/16\" EOF systemctl daemon-reload && systemctl restart docker docker pull registry.k8s.io/pause:3.6","title":"Docker\u4ee3\u7406"},{"location":"istio/istio_basics.html","text":"Istio \u67b6\u6784\u6982\u8ff0 \u00b6 Istio\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u72ec\u7acb\u670d\u52a1\u7f51\u683c\uff0c\u662fEnvoy Data Plane\u7684\u63a7\u5236\u5e73\u9762\u5b9e\u73b0\uff0c\u53ef\u4e3a\u7528\u6237\u6210\u529f\u8fd0\u884c\u5206\u5e03\u5f0f\u5fae\u670d\u52a1\u67b6\u6784\u63d0\u4f9b\u6240\u9700\u7684\u57fa\u7840\u8bbe\u65bd\u3002Istio \u670d\u52a1\u7f51\u683c\u903b\u8f91\u4e0a\u5206\u4e3a\u6570\u636e\u5e73\u9762\u548c\u63a7\u5236\u5e73\u9762\u3002 \u6570\u636e\u5e73\u9762\uff1a\u670d\u52a1\u7f51\u683c\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7ba1\u7406\u5b9e\u4f8b\u4e4b\u95f4\u7684\u7f51\u7edc\u6d41\u91cf\u7684\u90e8\u5206\uff0c\u4ee5Sidecar\u7684\u5f62\u5f0f\u4e0e\u670d\u52a1\u8fdb\u7a0b\u8fd0\u884c\u5728\u4e00\u8d77 \u63a7\u5236\u5e73\u9762\uff1a\u751f\u6210\u548c\u90e8\u7f72\u63a7\u5236\u6570\u636e\u5e73\u9762\u884c\u4e3a\u7684\u76f8\u5173\u914d\u7f6e\uff0c\u4e3b\u8981\u7531Pilot\u3001Mixer\u3001Citadel \u548cGalley\u56db\u4e2a\u7ec4\u4ef6\u7ec4\u6210 API\u63a5\u53e3 \u547d\u4ee4\u884c\u754c\u9762 \u7528\u4e8e\u7ba1\u7406\u5e94\u7528\u7a0b\u5e8f\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762 Istio \u7cfb\u7edf\u67b6\u6784 \u00b6 \u7ec4\u4ef6 \u7a0b\u5e8f\u6587\u4ef6 \u529f\u80fd \u63a7\u5236\u5e73\u9762 Pilot istio-pilot \u7ba1\u7406\u548c\u914d\u7f6e\u90e8\u7f72\u5728Istio\u670d\u52a1\u7f51\u683c\u4e2d\u7684\u6240\u6709Envoy\u4ee3\u7406\u5b9e\u4f8b Mixer istio-telemetry istio-policy \u8d1f\u8d23\u6536\u96c6\u9065\u6d4b\u6570\u636e \u8d1f\u8d23\u6267\u884c\u8bbf\u95ee\u7b56\u7565\u548c\u7ba1\u7406\u914d\u989d Citadel \u8d1f\u8d23\u670d\u52a1\u7684\u79c1\u94a5\u548c\u6570\u5b57\u8bc1\u4e66\u7ba1\u7406\uff0c\u7528\u4e8e\u63d0\u4f9b\u81ea\u52a8\u751f\u6210\u3001\u5206\u53d1\u3001\u8f6e\u6362\u53ca\u64a4\u9500\u79c1\u94a5\u548c\u6570\u636e\u8bc1\u4e66\u7684\u529f\u80fd Citadel \u8d1f\u8d23\u670d\u52a1\u7684\u79c1\u94a5\u548c\u6570\u5b57\u8bc1\u4e66\u7ba1\u7406\uff0c\u7528\u4e8e\u63d0\u4f9b\u81ea\u52a8\u751f\u6210\u3001\u5206\u53d1\u3001\u8f6e\u6362\u53ca\u64a4\u9500\u79c1\u94a5\u548c\u6570\u636e\u8bc1\u4e66\u7684\u529f\u80fd Galley \u8d1f\u8d23\u5411Istio\u63a7\u5236\u5e73\u9762\u7684\u5176\u5b83\u7ec4\u4ef6\u63d0\u4f9b\u652f\u6491\u529f\u80fd\uff0c\u5b83\u6838\u9a8c\u8fdb\u5165\u7f51\u683c\u7684\u914d\u7f6e\u4fe1\u606f\u7684\u683c\u5f0f\u548c\u5185\u5bb9\u7684\u6b63\u786e\u6027\uff0c\u5e76\u5c06\u8fd9\u4e9b\u914d\u7f6e\u4fe1\u606f\u63d0\u4f9b\u7ed9Pilot\u548cMixer \u8fd9\u662f\u8001\u7248\u672c\u5fae\u67b6\u6784\uff0c\u65b0\u7248\u672c \u4f7f\u7528 istiod \u5355\u4f53\u7a0b\u5e8f\u5b9e\u73b0\u4e0a\u8ff0\u529f\u80fd Ingress Gateways : \u53cd\u5411\u4ee3\u7406\uff0c\u7528\u4e8e\u5c06Istio\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u76d1\u63a7\u548c\u8def\u7531\u89c4\u5219\uff09\u5e94\u7528\u4e8e\u8fdb\u5165\u670d\u52a1\u7f51\u683c\u7684\u6d41\u91cf Engress Gateways \uff1a\u6b63\u5411\u4ee3\u7406\uff0c\u4ee3\u7406\u5e94\u7528\u6d41\u91cf\u8bbf\u95ee\u7f51\u683c\u5185\u5176\u4ed6\u5e94\u7528 Istiod \uff1a\u63d0\u4f9b\u670d\u52a1\u53d1\u73b0\u3001\u914d\u7f6e\u548c\u8bc1\u4e66\u7ba1\u7406 Istio\u7684\u6240\u6709\u8def\u7531\u89c4\u5219\u548c\u63a7\u5236\u7b56\u7565\u90fd\u57fa\u4e8eKubernetes CRD\u5b9e\u73b0\uff0c\u4e8e\u662f\uff0c\u5176\u5404\u79cd\u914d\u7f6e\u7b56\u7565\u7684\u5b9a\u4e49\u4e5f\u90fd\u4fdd\u5b58\u4e8ekube-apiserver\u540e\u7aef\u7684\u5b58\u50a8etcd\u4e2d\uff1b - kube-apiserver\u4e5f\u5c31\u662fIstio\u7684APIServer - Galley\u8d1f\u8d23\u4ecekube-apiserver\u52a0\u8f7d\u914d\u7f6e\u5e76\u8fdb\u884c\u5206\u53d1 Istio\u63d0\u4f9b\u4e86\u5927\u91cf\u7684CRD\uff0c\u5b83\u4eec\u5927\u4f53\u53ef\u5206\u4e3a\u5982\u4e0b\u51e0\u7c7b: \u7c7b\u578b \u5b9e\u73b0\u7ec4\u4ef6 \u8d44\u6e90 \u529f\u80fd Network Pilot VirtualService DestinationRule Gateway ServiceEntry EnvoyFilte \u5b9e\u73b0\u6d41\u91cf\u6cbb\u7406 Authentication Citadel Policy MeshPolicy \u5b9e\u73b0\u7b56\u7565\u8ba4\u8bc1 Config Mixer httpapispecbindings.config.istio.io httpapispecs.config.istio.io \u5b9e\u73b0Mixer\u7684\u5404\u79cd\u914d\u7f6e\u9700\u6c42 \u5feb\u901f\u90e8\u7f72Istio \u00b6 \u524d\u63d0\uff1a\u51c6\u5907\u597dkubernetes\u96c6\u7fa4 kubertes: v1.23.5 istio: v1.12.1 \u4e0b\u8f7distioctl\u53ca\u76f8\u5173\u7684\u5b89\u88c5\u6587\u4ef6\u548c\u793a\u4f8b\u6587\u4ef6 ~# wget https://github.com/istio/istio/releases/download/1.12.1/istio-1.12.1-linux-amd64.tar.gz ~# tar -xf istio-1.12.1-linux-amd64.tar.gz -C /usr/local/ ~# ln -sv /usr/local/istio-1.12.1/ /usr/local/istio ~# cp /usr/local/istio/bin/istioctl /usr/bin/ ~# istioctl version no running Istio pods in \"istio-system\" 1.12.1 \u5185\u7f6e\u914d\u7f6e\u6863\u6848 \u8bf4\u660e \u63a8\u8350\u4f7f\u7528 istio-egressgateway istio-ingressgateway istiod default \u6839\u636eIstioOperator API\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u542f\u7528\u76f8\u5173\u7684\u7ec4\u4ef6\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883 \u2705 \u2705 \u2705 demo \u4f1a\u90e8\u7f72\u8f83\u591a\u7684\u7ec4\u4ef6\uff0c\u65e8\u5728\u6f14\u793aistio\u7684\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u8fd0\u884cBookInfo\u4e00\u7c7b\u7684\u5e94\u7528\u7a0b\u5e8f \u2705 \u2705 \u2705 \u2705 minimal \u7c7b\u4f3c\u4e8edefault profile\uff0c\u4f46\u4ec5\u90e8\u7f72\u63a7\u5236\u5e73\u53f0\u7ec4\u4ef6 \u2705 remote \u7528\u4e8e\u914d\u7f6e\u5171\u4eabControl Plance\u7684\u591a\u96c6\u7fa4\u73af\u5883 empty \u4e0d\u90e8\u7f72\u4efb\u4f55\u7ec4\u4ef6\uff0c\u540c\u5728\u5e2e\u52a9\u7528\u6237\u5728\u81ea\u5b9a\u4e49profile\u662f\u751f\u6210\u57fa\u7840\u7684\u914d\u7f6e\u4fe1\u606f preview \u5305\u542b\u9884\u89c8\u6027\u914d\u7f6e\u7684profile\uff0c\u7528\u4e8e\u63a2\u7d22istio\u65b0\u529f\u80fd\uff0c\u4f46\u4e0d\u4fdd\u8bc1\u7a33\u5b9a\u6027\u3001\u5b89\u5168\u6027\u548c\u6027\u80fd \u2757\ufe0f \u2705 \u2705 \u67e5\u770b\u5185\u5efa profile ~# istioctl profile list Istio configuration profiles: default demo empty external minimal openshift preview remote \u67e5\u770b\u67d0profile\u8be6\u7ec6\u4fe1\u606f ~# istioctl profile dump demo \u5b89\u88c5istio\u63a7\u5236\u5e73\u9762 ~# istioctl install --set profile=demo -y \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Making this installation the default for injection and validation. Thank you for installing Istio 1.12. Please take a few minutes to tell us about your install/upgrade experience! https://forms.gle/FegQbc9UvePd4Z9z7 ~# kubectl get pod -n istio-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES istio-egressgateway-65bdddf685-6fk64 1/1 Running 0 117s 10.244.3.15 k8s-worker03 <none> <none> istio-ingressgateway-7b545cdbc7-cmpx4 1/1 Running 0 117s 10.244.1.14 k8s-worker01 <none> <none> istiod-864977fd6c-lf7hr 1/1 Running 0 2m59s 10.244.3.14 k8s-worker03 <none> <none>``` \u90e8\u7f72addons ~# cd /usr/local/istio/ /usr/local/istio# kubectl apply -f samples/addons/ /usr/local/istio# kubectl get pod -n istio-system -w NAME READY STATUS RESTARTS AGE grafana-5fb899f96-7q7cq 1/1 Running 0 4m5s istio-egressgateway-65bdddf685-6fk64 1/1 Running 0 12m istio-ingressgateway-7b545cdbc7-cmpx4 1/1 Running 0 12m istiod-864977fd6c-lf7hr 1/1 Running 0 13m jaeger-d7849fb76-5xql5 1/1 Running 0 4m5s kiali-c9d6f75d5-p6tm5 1/1 Running 0 4m5s prometheus-d7df8c957-r95fw 2/2 Running 0 4m4s \u6807\u8bb0\u540d\u79f0\u7a7a\u95f4\u81ea\u52a8\u6ce8\u5165sidercar ~# kubectl label namespace default istio-injection=enabled ~# kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 --replicas=3 deployment.apps/demoapp created ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demoapp-5748b7ccfc-g96k7 2/2 Running 0 2m49s 10.244.1.18 k8s-worker01 <none> <none> demoapp-5748b7ccfc-kzjg6 2/2 Running 0 2m49s 10.244.2.17 k8s-worker02 <none> <none> demoapp-5748b7ccfc-s947x 2/2 Running 0 2m49s 10.244.3.20 k8s-worker03 <none> <none> ~# kubectl exec -it demoapp-5748b7ccfc-s947x -- curl localhost:15000/listeners 750a3fbc-694b-4573-a86b-5a7d2e5b5075::0.0.0.0:15090 914ef458-da0d-49c4-8b30-11e7de70fd11::0.0.0.0:15021 10.101.188.140_443::10.101.188.140:443 10.106.26.93_443::10.106.26.93:443 10.106.26.93_31400::10.106.26.93:31400 10.96.0.1_443::10.96.0.1:443 10.96.0.10_53::10.96.0.10:53 10.101.188.140_15012::10.101.188.140:15012 10.100.191.210_443::10.100.191.210:443 10.106.26.93_15443::10.106.26.93:15443 0.0.0.0_15014::0.0.0.0:15014 10.100.9.63_14250::10.100.9.63:14250 10.99.69.60_3000::10.99.69.60:3000 10.100.9.63_14268::10.100.9.63:14268 0.0.0.0_20001::0.0.0.0:20001 10.98.71.107_80::10.98.71.107:80 0.0.0.0_15010::0.0.0.0:15010 10.106.26.93_15021::10.106.26.93:15021 0.0.0.0_80::0.0.0.0:80 0.0.0.0_9411::0.0.0.0:9411 0.0.0.0_9090::0.0.0.0:9090 10.96.0.10_9153::10.96.0.10:9153 0.0.0.0_16685::0.0.0.0:16685 virtualOutbound::0.0.0.0:15001 virtualInbound::0.0.0.0:15006 ~# kubectl exec -it demoapp-5748b7ccfc-s947x -- curl localhost:15000/clusters \u5378\u8f7dIstio kubectl delete -f samples/addons istioctl x uninstall -y --purge kubectl label namespace default istio-injection- kubectl delete namespace istio-system Istioctl \u547d\u4ee4 \u00b6 \u67e5\u770b\u914d\u7f6e\u4e0b\u53d1\u7684\u72b6\u6001 istioctl proxy-status [<type>/]<name>[.<namespace>] [flags] ~# istioctl proxy-status NAME CDS LDS EDS RDS ISTIOD VERSION demoapp-5748b7ccfc-g96k7.default SYNCED SYNCED SYNCED SYNCED istiod-864977fd6c-lf7hr 1.12.1 demoapp-5748b7ccfc-kzjg6.default SYNCED SYNCED SYNCED SYNCED istiod-864977fd6c-lf7hr 1.12.1 demoapp-5748b7ccfc-s947x.default SYNCED SYNCED SYNCED SYNCED istiod-864977fd6c-lf7hr 1.12.1 istio-egressgateway-65bdddf685-6fk64.istio-system SYNCED SYNCED SYNCED NOT SENT istiod-864977fd6c-lf7hr 1.12.1 istio-ingressgateway-7b545cdbc7-cmpx4.istio-system SYNCED SYNCED SYNCED NOT SENT istiod-864977fd6c-lf7hr 1.12.1 root@k8s-master01:~# \u67e5\u770b\u914d\u7f6e\u4fe1\u606f istioctl proxy-config <clusters|listeners|routes|endpoints|bootstrap|log|secret> <pod-name[.namespace]> ~# istioctl proxy-config clusters demoapp-5748b7ccfc-g96k7.default --port 80 SERVICE FQDN PORT SUBSET DIRECTION TYPE DESTINATION RULE istio-egressgateway.istio-system.svc.cluster.local 80 - outbound EDS istio-ingressgateway.istio-system.svc.cluster.local 80 - outbound EDS myapp.default.svc.cluster.local 80 - outbound EDS tracing.istio-system.svc.cluster.local 80 - outbound EDS \u6d41\u91cf\u6cbb\u7406 \u00b6","title":"Istio\u67b6\u6784\u6982\u8ff0"},{"location":"istio/istio_basics.html#istio","text":"Istio\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u72ec\u7acb\u670d\u52a1\u7f51\u683c\uff0c\u662fEnvoy Data Plane\u7684\u63a7\u5236\u5e73\u9762\u5b9e\u73b0\uff0c\u53ef\u4e3a\u7528\u6237\u6210\u529f\u8fd0\u884c\u5206\u5e03\u5f0f\u5fae\u670d\u52a1\u67b6\u6784\u63d0\u4f9b\u6240\u9700\u7684\u57fa\u7840\u8bbe\u65bd\u3002Istio \u670d\u52a1\u7f51\u683c\u903b\u8f91\u4e0a\u5206\u4e3a\u6570\u636e\u5e73\u9762\u548c\u63a7\u5236\u5e73\u9762\u3002 \u6570\u636e\u5e73\u9762\uff1a\u670d\u52a1\u7f51\u683c\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7ba1\u7406\u5b9e\u4f8b\u4e4b\u95f4\u7684\u7f51\u7edc\u6d41\u91cf\u7684\u90e8\u5206\uff0c\u4ee5Sidecar\u7684\u5f62\u5f0f\u4e0e\u670d\u52a1\u8fdb\u7a0b\u8fd0\u884c\u5728\u4e00\u8d77 \u63a7\u5236\u5e73\u9762\uff1a\u751f\u6210\u548c\u90e8\u7f72\u63a7\u5236\u6570\u636e\u5e73\u9762\u884c\u4e3a\u7684\u76f8\u5173\u914d\u7f6e\uff0c\u4e3b\u8981\u7531Pilot\u3001Mixer\u3001Citadel \u548cGalley\u56db\u4e2a\u7ec4\u4ef6\u7ec4\u6210 API\u63a5\u53e3 \u547d\u4ee4\u884c\u754c\u9762 \u7528\u4e8e\u7ba1\u7406\u5e94\u7528\u7a0b\u5e8f\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762","title":"Istio \u67b6\u6784\u6982\u8ff0"},{"location":"istio/istio_basics.html#istio_1","text":"\u7ec4\u4ef6 \u7a0b\u5e8f\u6587\u4ef6 \u529f\u80fd \u63a7\u5236\u5e73\u9762 Pilot istio-pilot \u7ba1\u7406\u548c\u914d\u7f6e\u90e8\u7f72\u5728Istio\u670d\u52a1\u7f51\u683c\u4e2d\u7684\u6240\u6709Envoy\u4ee3\u7406\u5b9e\u4f8b Mixer istio-telemetry istio-policy \u8d1f\u8d23\u6536\u96c6\u9065\u6d4b\u6570\u636e \u8d1f\u8d23\u6267\u884c\u8bbf\u95ee\u7b56\u7565\u548c\u7ba1\u7406\u914d\u989d Citadel \u8d1f\u8d23\u670d\u52a1\u7684\u79c1\u94a5\u548c\u6570\u5b57\u8bc1\u4e66\u7ba1\u7406\uff0c\u7528\u4e8e\u63d0\u4f9b\u81ea\u52a8\u751f\u6210\u3001\u5206\u53d1\u3001\u8f6e\u6362\u53ca\u64a4\u9500\u79c1\u94a5\u548c\u6570\u636e\u8bc1\u4e66\u7684\u529f\u80fd Citadel \u8d1f\u8d23\u670d\u52a1\u7684\u79c1\u94a5\u548c\u6570\u5b57\u8bc1\u4e66\u7ba1\u7406\uff0c\u7528\u4e8e\u63d0\u4f9b\u81ea\u52a8\u751f\u6210\u3001\u5206\u53d1\u3001\u8f6e\u6362\u53ca\u64a4\u9500\u79c1\u94a5\u548c\u6570\u636e\u8bc1\u4e66\u7684\u529f\u80fd Galley \u8d1f\u8d23\u5411Istio\u63a7\u5236\u5e73\u9762\u7684\u5176\u5b83\u7ec4\u4ef6\u63d0\u4f9b\u652f\u6491\u529f\u80fd\uff0c\u5b83\u6838\u9a8c\u8fdb\u5165\u7f51\u683c\u7684\u914d\u7f6e\u4fe1\u606f\u7684\u683c\u5f0f\u548c\u5185\u5bb9\u7684\u6b63\u786e\u6027\uff0c\u5e76\u5c06\u8fd9\u4e9b\u914d\u7f6e\u4fe1\u606f\u63d0\u4f9b\u7ed9Pilot\u548cMixer \u8fd9\u662f\u8001\u7248\u672c\u5fae\u67b6\u6784\uff0c\u65b0\u7248\u672c \u4f7f\u7528 istiod \u5355\u4f53\u7a0b\u5e8f\u5b9e\u73b0\u4e0a\u8ff0\u529f\u80fd Ingress Gateways : \u53cd\u5411\u4ee3\u7406\uff0c\u7528\u4e8e\u5c06Istio\u529f\u80fd\uff08\u4f8b\u5982\uff0c\u76d1\u63a7\u548c\u8def\u7531\u89c4\u5219\uff09\u5e94\u7528\u4e8e\u8fdb\u5165\u670d\u52a1\u7f51\u683c\u7684\u6d41\u91cf Engress Gateways \uff1a\u6b63\u5411\u4ee3\u7406\uff0c\u4ee3\u7406\u5e94\u7528\u6d41\u91cf\u8bbf\u95ee\u7f51\u683c\u5185\u5176\u4ed6\u5e94\u7528 Istiod \uff1a\u63d0\u4f9b\u670d\u52a1\u53d1\u73b0\u3001\u914d\u7f6e\u548c\u8bc1\u4e66\u7ba1\u7406 Istio\u7684\u6240\u6709\u8def\u7531\u89c4\u5219\u548c\u63a7\u5236\u7b56\u7565\u90fd\u57fa\u4e8eKubernetes CRD\u5b9e\u73b0\uff0c\u4e8e\u662f\uff0c\u5176\u5404\u79cd\u914d\u7f6e\u7b56\u7565\u7684\u5b9a\u4e49\u4e5f\u90fd\u4fdd\u5b58\u4e8ekube-apiserver\u540e\u7aef\u7684\u5b58\u50a8etcd\u4e2d\uff1b - kube-apiserver\u4e5f\u5c31\u662fIstio\u7684APIServer - Galley\u8d1f\u8d23\u4ecekube-apiserver\u52a0\u8f7d\u914d\u7f6e\u5e76\u8fdb\u884c\u5206\u53d1 Istio\u63d0\u4f9b\u4e86\u5927\u91cf\u7684CRD\uff0c\u5b83\u4eec\u5927\u4f53\u53ef\u5206\u4e3a\u5982\u4e0b\u51e0\u7c7b: \u7c7b\u578b \u5b9e\u73b0\u7ec4\u4ef6 \u8d44\u6e90 \u529f\u80fd Network Pilot VirtualService DestinationRule Gateway ServiceEntry EnvoyFilte \u5b9e\u73b0\u6d41\u91cf\u6cbb\u7406 Authentication Citadel Policy MeshPolicy \u5b9e\u73b0\u7b56\u7565\u8ba4\u8bc1 Config Mixer httpapispecbindings.config.istio.io httpapispecs.config.istio.io \u5b9e\u73b0Mixer\u7684\u5404\u79cd\u914d\u7f6e\u9700\u6c42","title":"Istio \u7cfb\u7edf\u67b6\u6784"},{"location":"istio/istio_basics.html#istio_2","text":"\u524d\u63d0\uff1a\u51c6\u5907\u597dkubernetes\u96c6\u7fa4 kubertes: v1.23.5 istio: v1.12.1 \u4e0b\u8f7distioctl\u53ca\u76f8\u5173\u7684\u5b89\u88c5\u6587\u4ef6\u548c\u793a\u4f8b\u6587\u4ef6 ~# wget https://github.com/istio/istio/releases/download/1.12.1/istio-1.12.1-linux-amd64.tar.gz ~# tar -xf istio-1.12.1-linux-amd64.tar.gz -C /usr/local/ ~# ln -sv /usr/local/istio-1.12.1/ /usr/local/istio ~# cp /usr/local/istio/bin/istioctl /usr/bin/ ~# istioctl version no running Istio pods in \"istio-system\" 1.12.1 \u5185\u7f6e\u914d\u7f6e\u6863\u6848 \u8bf4\u660e \u63a8\u8350\u4f7f\u7528 istio-egressgateway istio-ingressgateway istiod default \u6839\u636eIstioOperator API\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u542f\u7528\u76f8\u5173\u7684\u7ec4\u4ef6\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883 \u2705 \u2705 \u2705 demo \u4f1a\u90e8\u7f72\u8f83\u591a\u7684\u7ec4\u4ef6\uff0c\u65e8\u5728\u6f14\u793aistio\u7684\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u8fd0\u884cBookInfo\u4e00\u7c7b\u7684\u5e94\u7528\u7a0b\u5e8f \u2705 \u2705 \u2705 \u2705 minimal \u7c7b\u4f3c\u4e8edefault profile\uff0c\u4f46\u4ec5\u90e8\u7f72\u63a7\u5236\u5e73\u53f0\u7ec4\u4ef6 \u2705 remote \u7528\u4e8e\u914d\u7f6e\u5171\u4eabControl Plance\u7684\u591a\u96c6\u7fa4\u73af\u5883 empty \u4e0d\u90e8\u7f72\u4efb\u4f55\u7ec4\u4ef6\uff0c\u540c\u5728\u5e2e\u52a9\u7528\u6237\u5728\u81ea\u5b9a\u4e49profile\u662f\u751f\u6210\u57fa\u7840\u7684\u914d\u7f6e\u4fe1\u606f preview \u5305\u542b\u9884\u89c8\u6027\u914d\u7f6e\u7684profile\uff0c\u7528\u4e8e\u63a2\u7d22istio\u65b0\u529f\u80fd\uff0c\u4f46\u4e0d\u4fdd\u8bc1\u7a33\u5b9a\u6027\u3001\u5b89\u5168\u6027\u548c\u6027\u80fd \u2757\ufe0f \u2705 \u2705 \u67e5\u770b\u5185\u5efa profile ~# istioctl profile list Istio configuration profiles: default demo empty external minimal openshift preview remote \u67e5\u770b\u67d0profile\u8be6\u7ec6\u4fe1\u606f ~# istioctl profile dump demo \u5b89\u88c5istio\u63a7\u5236\u5e73\u9762 ~# istioctl install --set profile=demo -y \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Making this installation the default for injection and validation. Thank you for installing Istio 1.12. Please take a few minutes to tell us about your install/upgrade experience! https://forms.gle/FegQbc9UvePd4Z9z7 ~# kubectl get pod -n istio-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES istio-egressgateway-65bdddf685-6fk64 1/1 Running 0 117s 10.244.3.15 k8s-worker03 <none> <none> istio-ingressgateway-7b545cdbc7-cmpx4 1/1 Running 0 117s 10.244.1.14 k8s-worker01 <none> <none> istiod-864977fd6c-lf7hr 1/1 Running 0 2m59s 10.244.3.14 k8s-worker03 <none> <none>``` \u90e8\u7f72addons ~# cd /usr/local/istio/ /usr/local/istio# kubectl apply -f samples/addons/ /usr/local/istio# kubectl get pod -n istio-system -w NAME READY STATUS RESTARTS AGE grafana-5fb899f96-7q7cq 1/1 Running 0 4m5s istio-egressgateway-65bdddf685-6fk64 1/1 Running 0 12m istio-ingressgateway-7b545cdbc7-cmpx4 1/1 Running 0 12m istiod-864977fd6c-lf7hr 1/1 Running 0 13m jaeger-d7849fb76-5xql5 1/1 Running 0 4m5s kiali-c9d6f75d5-p6tm5 1/1 Running 0 4m5s prometheus-d7df8c957-r95fw 2/2 Running 0 4m4s \u6807\u8bb0\u540d\u79f0\u7a7a\u95f4\u81ea\u52a8\u6ce8\u5165sidercar ~# kubectl label namespace default istio-injection=enabled ~# kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 --replicas=3 deployment.apps/demoapp created ~# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES demoapp-5748b7ccfc-g96k7 2/2 Running 0 2m49s 10.244.1.18 k8s-worker01 <none> <none> demoapp-5748b7ccfc-kzjg6 2/2 Running 0 2m49s 10.244.2.17 k8s-worker02 <none> <none> demoapp-5748b7ccfc-s947x 2/2 Running 0 2m49s 10.244.3.20 k8s-worker03 <none> <none> ~# kubectl exec -it demoapp-5748b7ccfc-s947x -- curl localhost:15000/listeners 750a3fbc-694b-4573-a86b-5a7d2e5b5075::0.0.0.0:15090 914ef458-da0d-49c4-8b30-11e7de70fd11::0.0.0.0:15021 10.101.188.140_443::10.101.188.140:443 10.106.26.93_443::10.106.26.93:443 10.106.26.93_31400::10.106.26.93:31400 10.96.0.1_443::10.96.0.1:443 10.96.0.10_53::10.96.0.10:53 10.101.188.140_15012::10.101.188.140:15012 10.100.191.210_443::10.100.191.210:443 10.106.26.93_15443::10.106.26.93:15443 0.0.0.0_15014::0.0.0.0:15014 10.100.9.63_14250::10.100.9.63:14250 10.99.69.60_3000::10.99.69.60:3000 10.100.9.63_14268::10.100.9.63:14268 0.0.0.0_20001::0.0.0.0:20001 10.98.71.107_80::10.98.71.107:80 0.0.0.0_15010::0.0.0.0:15010 10.106.26.93_15021::10.106.26.93:15021 0.0.0.0_80::0.0.0.0:80 0.0.0.0_9411::0.0.0.0:9411 0.0.0.0_9090::0.0.0.0:9090 10.96.0.10_9153::10.96.0.10:9153 0.0.0.0_16685::0.0.0.0:16685 virtualOutbound::0.0.0.0:15001 virtualInbound::0.0.0.0:15006 ~# kubectl exec -it demoapp-5748b7ccfc-s947x -- curl localhost:15000/clusters \u5378\u8f7dIstio kubectl delete -f samples/addons istioctl x uninstall -y --purge kubectl label namespace default istio-injection- kubectl delete namespace istio-system","title":"\u5feb\u901f\u90e8\u7f72Istio"},{"location":"istio/istio_basics.html#istioctl","text":"\u67e5\u770b\u914d\u7f6e\u4e0b\u53d1\u7684\u72b6\u6001 istioctl proxy-status [<type>/]<name>[.<namespace>] [flags] ~# istioctl proxy-status NAME CDS LDS EDS RDS ISTIOD VERSION demoapp-5748b7ccfc-g96k7.default SYNCED SYNCED SYNCED SYNCED istiod-864977fd6c-lf7hr 1.12.1 demoapp-5748b7ccfc-kzjg6.default SYNCED SYNCED SYNCED SYNCED istiod-864977fd6c-lf7hr 1.12.1 demoapp-5748b7ccfc-s947x.default SYNCED SYNCED SYNCED SYNCED istiod-864977fd6c-lf7hr 1.12.1 istio-egressgateway-65bdddf685-6fk64.istio-system SYNCED SYNCED SYNCED NOT SENT istiod-864977fd6c-lf7hr 1.12.1 istio-ingressgateway-7b545cdbc7-cmpx4.istio-system SYNCED SYNCED SYNCED NOT SENT istiod-864977fd6c-lf7hr 1.12.1 root@k8s-master01:~# \u67e5\u770b\u914d\u7f6e\u4fe1\u606f istioctl proxy-config <clusters|listeners|routes|endpoints|bootstrap|log|secret> <pod-name[.namespace]> ~# istioctl proxy-config clusters demoapp-5748b7ccfc-g96k7.default --port 80 SERVICE FQDN PORT SUBSET DIRECTION TYPE DESTINATION RULE istio-egressgateway.istio-system.svc.cluster.local 80 - outbound EDS istio-ingressgateway.istio-system.svc.cluster.local 80 - outbound EDS myapp.default.svc.cluster.local 80 - outbound EDS tracing.istio-system.svc.cluster.local 80 - outbound EDS","title":"Istioctl \u547d\u4ee4"},{"location":"istio/istio_basics.html#_1","text":"","title":"\u6d41\u91cf\u6cbb\u7406"},{"location":"k8s/helm_pkg_manager.html","text":"Helm \u5305\u7ba1\u7406\u5668 \u00b6 \u5b89\u88c5 \u00b6 ~# wget https://get.helm.sh/helm-v3.14.3-linux-amd64.tar.gz ~# tar -xf helm-v3.14.3-linux-amd64.tar.gz ~# cp linux-amd64/helm /usr/bin/ ~# helm version version.BuildInfo{Version:\"v3.14.3\", GitCommit:\"f03cc04caaa8f6d7c3e67cf918929150cf6f3f12\", GitTreeState:\"clean\", GoVersion:\"go1.21.7\"}","title":"Helm \u5305\u7ba1\u7406\u5668"},{"location":"k8s/helm_pkg_manager.html#helm","text":"","title":"Helm \u5305\u7ba1\u7406\u5668"},{"location":"k8s/helm_pkg_manager.html#_1","text":"~# wget https://get.helm.sh/helm-v3.14.3-linux-amd64.tar.gz ~# tar -xf helm-v3.14.3-linux-amd64.tar.gz ~# cp linux-amd64/helm /usr/bin/ ~# helm version version.BuildInfo{Version:\"v3.14.3\", GitCommit:\"f03cc04caaa8f6d7c3e67cf918929150cf6f3f12\", GitTreeState:\"clean\", GoVersion:\"go1.21.7\"}","title":"\u5b89\u88c5"},{"location":"k8s/install_docker_k8s.html","text":"Kubeadm \u5b89\u88c5 Kubernetes \u00b6 \u57fa\u4e8e Kubeadm \u90e8\u7f72Kubernetes\u96c6\u7fa4\u3002\u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 20.04 LTS\uff0c\u7528\u5230\u7684\u5404\u76f8\u5173\u7a0b\u5e8f\u7248\u672c\u5982\u4e0b\uff1a kubernetes: v1.28.6 docker: 20.10.22 cri-dockerd: v0.3.8 cni: flannel \u73af\u5883\u8bf4\u660e \u4e3b\u673a\u5730\u5740 \u8282\u70b9\u540d\u79f0 \u89d2\u8272 192.168.122.11 k8s-master01 master 192.168.122.21 k8s-worker01 worker 192.168.122.22 k8s-worker02 worker 192.168.122.23 k8s-worker03 worker \u4e00\u3001\u51c6\u5907\u865a\u62df\u673a \u00b6 1.1 \u521d\u59cb\u5316\u865a\u62df\u673a \u00b6 \u5141\u8bb8root\u7528\u6237\u8fdc\u7a0b\u767b\u9646 ~# echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config ~# systemctl restart sshd ~# passwd root \u66f4\u65b0apt\u6e90\u4e3a\u963f\u91cc\u6e90 cat > /etc/apt/sources.list <<\"EOF\" deb https://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse # deb https://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse # deb-src https://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse EOF \u5173\u95ed\u9632\u706b\u5899 ~# ufw disable \u5173\u95edswap\u5206\u533a ~# sed -ri 's@/.*swap.*@# &@' /etc/fstab && swapoff -a \u8c03\u6574\u65f6\u533a\u548c\u65f6\u95f4\u540c\u6b65 ~# sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ~# date -R ~# sudo apt update ~# sudo apt install -y ntpdate ~# /usr/sbin/ntpdate ntp.aliyun.com 2&>1 /dev/null ~# crontab -l */3 * * * * /usr/sbin/ntpdate ntp.aliyun.com 2&>1 /dev/null \u6253\u5f00\u5185\u6838\u8f6c\u53d1 ~# cat > /etc/sysctl.d/k8s.conf <<EOF net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-arptables = 1 net.ipv4.tcp_tw_reuse = 0 net.core.somaxconn = 32768 net.netfilter.nf_conntrack_max=1000000 vm.swappiness = 0 vm.max_map_count=655360 fs.file-max=6553600 EOF ~# sysctl -p ~# cat >> /etc/modules-load.d/k8s.conf << \"EOF\" overlay br_netfilter EOF ~# sudo modprobe overlay ~# sudo modprobe br_netfilter - \u52a0\u8f7d IPVS \u6a21\u5757 \u00b6 apt install ipvsadm ipset -y cat > /etc/modules-load.d/ipvs.conf << \"EOF\" #!/bin/bash ipvs_mods_dir=\"/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs\" for i in $(ls $ipvs_mods_dir | grep -o \"^[^.]*\"); do /sbin/modinfo -F filename $i &> /dev/null if [ $? -eq 0 ]; then /sbin/modprobe $i fi done EOF bash /etc/modules-load.d/ipvs.conf ~# lsmod | grep -e ip_vs -e nf_conntrack_ipv4 \u5173\u673a ~# init 0 1.2. \u514b\u9686\u865a\u62df\u673a \u00b6 \u67e5\u770b\u6a21\u677f\u865a\u62df\u673a\u72b6\u6001 ~]# virsh list --all Id Name State ---------------------------------------------------- - ubuntu20.04 shut off \u514b\u9686\u865a\u62df\u673a ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-master01 ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-worker01 ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-worker02 ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-worker03 \u4fee\u6539\u4e3b\u673aIP\u5730\u5740\u548c\u4e3b\u673a\u540d ~]# virt-sysprep \\ --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-master01 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.11@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-master01 ~]# virt-sysprep --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-worker01 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.21@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-worker01 ~]# virt-sysprep --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-worker02 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.22@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-worker02 ~]# virt-sysprep --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-worker03 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.23@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-worker03 1.3. \u542f\u52a8\u865a\u62df\u673a \u00b6 ~]# virsh start k8s-master01 ~]# virsh start k8s-worker01 ~]# virsh start k8s-worker02 ~]# virsh start k8s-worker03 \u4e8c\u3001\u5b89\u88c5\u96c6\u7fa4 \u00b6 2.1 \u4e3b\u673a\u540d\u89e3\u6790 \u00b6 ~# cat >> /etc/hosts <<EOF 192.168.122.11 k8s-master01 k8s-master01.linux.io 192.168.122.21 k8s-worker01 k8s-worker01.linux.io 192.168.122.22 k8s-worker02 k8s-worker02.linux.io 192.168.122.23 k8s-worker03 k8s-worker03.linux.io 192.168.122.11 k8s.linux.io EOF 2.2 \u5b89\u88c5DOCKER \u00b6 \u914d\u7f6e apt \u6e90 # step 1: \u5b89\u88c5\u5fc5\u8981\u7684\u4e00\u4e9b\u7cfb\u7edf\u5de5\u5177 sudo apt-get update sudo apt-get install ca-certificates curl gnupg # step 2: \u4fe1\u4efb Docker \u7684 GPG \u516c\u94a5 sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Step 3: \u5199\u5165\u8f6f\u4ef6\u6e90\u4fe1\u606f echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null \u5b89\u88c5docker sudo apt-get update sudo apt-cache madison docker-ce sudo apt install -y docker-ce=5:20.10.22~3-0~ubuntu-focal \u914d\u7f6edocker\u52a0\u901f\u5668 mkdir -pv /etc/docker sudo cat > /etc/docker/daemon.json <<-'EOF' { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [ \"https://docker.rainbond.cc\" ] } EOF systemctl restart docker 2.3 \u5b89\u88c5 cri-dockerd \u00b6 wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.8/cri-dockerd-0.3.8.amd64.tgz tar -xzvf cri-dockerd-0.3.8.amd64.tgz sudo install -m 0755 -o root -g root -t /usr/local/bin cri-dockerd/cri-dockerd wget https://raw.githubusercontent.com/Mirantis/cri-dockerd/master/packaging/systemd/cri-docker.service wget https://raw.githubusercontent.com/Mirantis/cri-dockerd/master/packaging/systemd/cri-docker.socket sudo install cri-docker.service /etc/systemd/system sudo install cri-docker.socket /etc/systemd/system sudo sed -i -e 's@/usr/bin/cri-dockerd@/usr/local/bin/cri-dockerd@' /etc/systemd/system/cri-docker.service sudo systemctl daemon-reload sudo systemctl enable --now cri-docker.socket sudo systemctl start cri-docker.service && systemctl status cri-docker.service 2.4 \u5b89\u88c5 kubeadm\u3001kubelet \u548c kubectl \u00b6 \u914d\u7f6ekubernetes\u6e90 apt-get update && apt-get install -y apt-transport-https sudo mkdir -m 755 /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/ /\" | tee /etc/apt/sources.list.d/kubernetes.list apt-get update apt-cache madison kubeadm apt install -y kubeadm=1.28.6-1.1 kubelet=1.28.6-1.1 kubectl=1.28.6-1.1 \u4e09\u3001\u521d\u59cb\u5316\u96c6\u7fa4 \u00b6 3.1 \u62c9\u53d6\u955c\u50cf \u00b6 ~]# kubeadm config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\ --kubernetes-version=1.28.6 \\ --cri-socket=unix:///var/run/cri-dockerd.sock [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.10-0 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.10.1 3.2 \u521d\u59cb\u5316 master \u00b6 \u914d\u7f6ecri-docker\u4e2d\u521d\u59cb\u5316\u5bb9\u5668 ~# cp /etc/systemd/system/cri-docker.service{,.bak} ~# vim /etc/systemd/system/cri-docker.service [Unit] ... [Service] Type=notify #ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// ExecStart=/usr/local/bin/cri-dockerd --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 --container-runtime-endpoint fd:// ~# systemctl daemon-reload && systemctl restart cri-docker.service && systemctl status cri-docker.service for i in k8s-worker01 k8s-worker02 k8s-worker03;do scp /etc/systemd/system/cri-docker.service root@$i:/etc/systemd/system/cri-docker.service ssh $i \"systemctl daemon-reload && systemctl restart cri-docker.service && systemctl status cri-docker.service\" done \u521d\u59cb\u5316master # \u6240\u6709\u8282\u70b9\u9700\u8981\u89e3\u6790 k8s.linux.io, \u6b64\u57df\u540d\u7528\u4e8e\u540e\u7eed\u6269\u5c55\u96c6\u7fa4\u4e3a\u9ad8\u53ef\u7528\u96c6\u7fa4 ~]# kubeadm init --kubernetes-version=v1.28.6 \\ --control-plane-endpoint=k8s.linux.io \\ --apiserver-advertise-address=0.0.0.0 \\ --pod-network-cidr=10.244.0.0/16 \\ --service-cidr=10.96.0.0/12 \\ --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\ --ignore-preflight-errors=Swap \\ --cri-socket=unix:///var/run/cri-dockerd.sock | tee kubeadm-init.log ... Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join k8s.linux.io:6443 --token 40zawh.qty5miole7ny5hf8 \\ --discovery-token-ca-cert-hash sha256:832dd459e1bf101d54f6ff80bec406f468aa9cd4b359c9536c845d696fdc8f21 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join k8s.linux.io:6443 --token 40zawh.qty5miole7ny5hf8 \\ --discovery-token-ca-cert-hash sha256:832dd459e1bf101d54f6ff80bec406f468aa9cd4b359c9536c845d696fdc8f21 \u914d\u7f6ekubectl ~# mkdir -p $HOME/.kube ~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config ~# sudo chown $(id -u):$(id -g) $HOME/.kube/config ~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 NotReady control-plane 113s v1.28.6 3.2 \u52a0\u5165 Worker \u00b6 kubeadm join k8s.linux.io:6443 --token 40zawh.qty5miole7ny5hf8 \\ --discovery-token-ca-cert-hash sha256:832dd459e1bf101d54f6ff80bec406f468aa9cd4b359c9536c845d696fdc8f21 \\ --cri-socket=unix:///var/run/cri-dockerd.sock ~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 NotReady control-plane 9m28s v1.28.6 k8s-worker01 NotReady <none> 2m33s v1.28.6 k8s-worker02 NotReady <none> 2m30s v1.28.6 k8s-worker03 NotReady <none> 2m27s v1.28.6 ~# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6554b8b87f-k9jxl 0/1 Pending 0 9m17s kube-system coredns-6554b8b87f-tfkv8 0/1 Pending 0 9m17s kube-system etcd-k8s-master01 1/1 Running 0 9m29s kube-system kube-apiserver-k8s-master01 1/1 Running 0 9m29s kube-system kube-controller-manager-k8s-master01 1/1 Running 0 9m29s kube-system kube-proxy-fk5jk 1/1 Running 0 2m40s kube-system kube-proxy-hzqdp 1/1 Running 0 9m17s kube-system kube-proxy-l8grw 1/1 Running 0 2m43s kube-system kube-proxy-vgn8b 1/1 Running 0 2m37s kube-system kube-scheduler-k8s-master01 1/1 Running 0 9m29s 3.3 \u5b89\u88c5\u7f51\u7edc\u63d2\u4ef6 \u00b6 \u5b89\u88c5 ~# wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml ~# grep -C 5 244 kube-flannel.yml } ] } net-conf.json: | { \"Network\": \"10.244.0.0/16\", \"EnableNFTables\": false, \"Backend\": { \"Type\": \"vxlan\" } } ~# grep image kube-flannel.yml image: docker.io/flannel/flannel:v0.26.1 image: docker.io/flannel/flannel-cni-plugin:v1.5.1-flannel2 image: docker.io/flannel/flannel:v0.26.1 ~# kubectl apply -f kube-flannel.yml ~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane 15m v1.28.6 k8s-worker01 Ready <none> 8m40s v1.28.6 k8s-worker02 Ready <none> 8m37s v1.28.6 k8s-worker03 Ready <none> 8m34s v1.28.6 ~# kubectl get pod -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-flannel kube-flannel-ds-h8zrm 1/1 Running 0 102s 192.168.122.21 k8s-worker01 <none> <none> kube-flannel kube-flannel-ds-l8cgv 1/1 Running 0 102s 192.168.122.23 k8s-worker03 <none> <none> kube-flannel kube-flannel-ds-q2nk2 1/1 Running 0 102s 192.168.122.11 k8s-master01 <none> <none> kube-flannel kube-flannel-ds-tg7r9 1/1 Running 0 102s 192.168.122.22 k8s-worker02 <none> <none> kube-system coredns-6554b8b87f-k9jxl 1/1 Running 0 15m 10.244.0.3 k8s-master01 <none> <none> kube-system coredns-6554b8b87f-tfkv8 1/1 Running 0 15m 10.244.0.2 k8s-master01 <none> <none> kube-system etcd-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-apiserver-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-controller-manager-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-proxy-fk5jk 1/1 Running 0 8m45s 192.168.122.22 k8s-worker02 <none> <none> kube-system kube-proxy-hzqdp 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-proxy-l8grw 1/1 Running 0 8m48s 192.168.122.21 k8s-worker01 <none> <none> kube-system kube-proxy-vgn8b 1/1 Running 0 8m42s 192.168.122.23 k8s-worker03 <none> <none> kube-system kube-scheduler-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> \u8c03\u6574\u4e3aipvs\u6a21\u5f0f kubectl edit cm kube-proxy -n kube-system mode: \"ipvs\" kubectl delete pod -n kube-system -l k8s-app=kube-proxy \u56db\u3001\u9a8c\u8bc1\u96c6\u7fa4 \u00b6 4.1 \u9a8c\u8bc1\u7f51\u7edc \u00b6 \u521b\u5efa Deployment \u548c Service \u8d44\u6e90 ~# kubectl create deployment myapp --image=ikubernetes/myapp:v1 --replicas=3 ~# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myapp-5d9c4b4647-7xmkx 1/1 Running 0 31s 10.244.2.2 k8s-worker02 <none> <none> myapp-5d9c4b4647-qrkdl 1/1 Running 0 31s 10.244.1.2 k8s-worker01 <none> <none> myapp-5d9c4b4647-w9lq7 1/1 Running 0 31s 10.244.3.2 k8s-worker03 <none> <none> ~# kubectl expose deployment/myapp --type=NodePort --port=80 --target-port=80 ~# kubectl get svc myapp -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR myapp NodePort 10.106.198.16 <none> 80:30940/TCP 25s app=myapp \u901a\u8fc7 Service \u6216\u8005 NodePort \u8d44\u6e90\u8f6e\u8be2 Pod ~# for i in `seq 5`;do curl 10.106.198.16/hostname.html;done myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-7xmkx ~]# for i in `seq 5`; do curl 192.168.122.11:30940/hostname.html;done myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-7xmkx \u6ce8\u610f\uff1aService IP \u548c NodePort \u4ec5\u662f\u7f51\u7edc\u89c4\u5219\uff0c\u6240\u4ee5\u4e0d\u652f\u6301ping\uff0c \u4f46\u662f\u652f\u6301telnet 4.1 \u9a8c\u8bc1DNS \u00b6 ~# kubectl get svc -n kube-system -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 29m k8s-app=kube-dns ~# kubectl run busybox --image=busybox:1.28 -- sleep 3600 ~# kubectl get pod/busybox NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 16s ~# kubectl exec -it busybox -- nslookup myapp.default.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: myapp.default.svc.cluster.local Address 1: 10.106.198.16 myapp.default.svc.cluster.local \u4e94\u3001\u63d2\u4ef6\u5b89\u88c5 \u00b6 5.1 Metrics-Server \u00b6 \u5b98\u7f51\uff1a https://github.com/kubernetes-sigs/metrics-server ~# wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml ~# vim components.yaml spec: containers: - args: - --cert-dir=/tmp - --secure-port=10250 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls ~# kubectl apply -f components.yaml ~# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 189m 4% 1766Mi 22% k8s-worker01 61m 1% 964Mi 12% k8s-worker02 59m 1% 913Mi 11% k8s-worker03 61m 1% 937Mi 12% ~# kubectl top pods NAME CPU(cores) MEMORY(bytes) busybox 0m 1Mi myapp-5d9c4b4647-4t4sp 0m 2Mi myapp-5d9c4b4647-fb4kh 0m 2Mi myapp-5d9c4b4647-zgqtw 0m 2Mi 5.2 Dashboard \u00b6 \u5b98\u7f51\uff1a https://github.com/kubernetes/dashboard \u90e8\u7f72 dashboard wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml kubectl apply -f recommended.yaml kubectl patch -n kubernetes-dashboard svc/kubernetes-dashboard -p '{\"spec\": {\"type\": \"NodePort\"}}' # kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.104.44.78 <none> 8000/TCP 17m kubernetes-dashboard NodePort 10.104.155.213 <none> 443:30106/TCP 17m \u521b\u5efa\u7528\u6237 cat admin-user.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard kubectl apply -f admin-user.yaml kubectl -n kubernetes-dashboard create token admin-user \u8bbf\u95ee \u5982\u679cgoogle\u4e0d\u5141\u8bb8\u8bbf\u95eehttps\uff0c\u8bf7\u5728\u8bbf\u95ee\u9875\u9762\u8f93\u5165 thisisunsafe \u89e3\u51b3","title":"\u57fa\u4e8ekubeadm\u5b89\u88c5kubernetes\u96c6\u7fa4"},{"location":"k8s/install_docker_k8s.html#kubeadm-kubernetes","text":"\u57fa\u4e8e Kubeadm \u90e8\u7f72Kubernetes\u96c6\u7fa4\u3002\u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 20.04 LTS\uff0c\u7528\u5230\u7684\u5404\u76f8\u5173\u7a0b\u5e8f\u7248\u672c\u5982\u4e0b\uff1a kubernetes: v1.28.6 docker: 20.10.22 cri-dockerd: v0.3.8 cni: flannel \u73af\u5883\u8bf4\u660e \u4e3b\u673a\u5730\u5740 \u8282\u70b9\u540d\u79f0 \u89d2\u8272 192.168.122.11 k8s-master01 master 192.168.122.21 k8s-worker01 worker 192.168.122.22 k8s-worker02 worker 192.168.122.23 k8s-worker03 worker","title":"Kubeadm \u5b89\u88c5 Kubernetes"},{"location":"k8s/install_docker_k8s.html#_1","text":"","title":"\u4e00\u3001\u51c6\u5907\u865a\u62df\u673a"},{"location":"k8s/install_docker_k8s.html#11","text":"\u5141\u8bb8root\u7528\u6237\u8fdc\u7a0b\u767b\u9646 ~# echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config ~# systemctl restart sshd ~# passwd root \u66f4\u65b0apt\u6e90\u4e3a\u963f\u91cc\u6e90 cat > /etc/apt/sources.list <<\"EOF\" deb https://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse # deb https://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse # deb-src https://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse EOF \u5173\u95ed\u9632\u706b\u5899 ~# ufw disable \u5173\u95edswap\u5206\u533a ~# sed -ri 's@/.*swap.*@# &@' /etc/fstab && swapoff -a \u8c03\u6574\u65f6\u533a\u548c\u65f6\u95f4\u540c\u6b65 ~# sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ~# date -R ~# sudo apt update ~# sudo apt install -y ntpdate ~# /usr/sbin/ntpdate ntp.aliyun.com 2&>1 /dev/null ~# crontab -l */3 * * * * /usr/sbin/ntpdate ntp.aliyun.com 2&>1 /dev/null \u6253\u5f00\u5185\u6838\u8f6c\u53d1 ~# cat > /etc/sysctl.d/k8s.conf <<EOF net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-arptables = 1 net.ipv4.tcp_tw_reuse = 0 net.core.somaxconn = 32768 net.netfilter.nf_conntrack_max=1000000 vm.swappiness = 0 vm.max_map_count=655360 fs.file-max=6553600 EOF ~# sysctl -p ~# cat >> /etc/modules-load.d/k8s.conf << \"EOF\" overlay br_netfilter EOF ~# sudo modprobe overlay ~# sudo modprobe br_netfilter","title":"1.1 \u521d\u59cb\u5316\u865a\u62df\u673a"},{"location":"k8s/install_docker_k8s.html#-ipvs","text":"apt install ipvsadm ipset -y cat > /etc/modules-load.d/ipvs.conf << \"EOF\" #!/bin/bash ipvs_mods_dir=\"/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs\" for i in $(ls $ipvs_mods_dir | grep -o \"^[^.]*\"); do /sbin/modinfo -F filename $i &> /dev/null if [ $? -eq 0 ]; then /sbin/modprobe $i fi done EOF bash /etc/modules-load.d/ipvs.conf ~# lsmod | grep -e ip_vs -e nf_conntrack_ipv4 \u5173\u673a ~# init 0","title":"- \u52a0\u8f7d IPVS \u6a21\u5757"},{"location":"k8s/install_docker_k8s.html#12","text":"\u67e5\u770b\u6a21\u677f\u865a\u62df\u673a\u72b6\u6001 ~]# virsh list --all Id Name State ---------------------------------------------------- - ubuntu20.04 shut off \u514b\u9686\u865a\u62df\u673a ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-master01 ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-worker01 ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-worker02 ~]# virt-clone --auto-clone -o ubuntu20.04 -n k8s-worker03 \u4fee\u6539\u4e3b\u673aIP\u5730\u5740\u548c\u4e3b\u673a\u540d ~]# virt-sysprep \\ --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-master01 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.11@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-master01 ~]# virt-sysprep --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-worker01 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.21@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-worker01 ~]# virt-sysprep --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-worker02 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.22@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-worker02 ~]# virt-sysprep --operations defaults,machine-id,-ssh-userdir,-lvm-uuids \\ --hostname k8s-worker03 \\ --run-command \"sed -i 's@192.168.122.7@192.168.122.23@g' /etc/netplan/00-installer-config.yaml && dpkg-reconfigure openssh-server\" \\ -d k8s-worker03","title":"1.2. \u514b\u9686\u865a\u62df\u673a"},{"location":"k8s/install_docker_k8s.html#13","text":"~]# virsh start k8s-master01 ~]# virsh start k8s-worker01 ~]# virsh start k8s-worker02 ~]# virsh start k8s-worker03","title":"1.3. \u542f\u52a8\u865a\u62df\u673a"},{"location":"k8s/install_docker_k8s.html#_2","text":"","title":"\u4e8c\u3001\u5b89\u88c5\u96c6\u7fa4"},{"location":"k8s/install_docker_k8s.html#21","text":"~# cat >> /etc/hosts <<EOF 192.168.122.11 k8s-master01 k8s-master01.linux.io 192.168.122.21 k8s-worker01 k8s-worker01.linux.io 192.168.122.22 k8s-worker02 k8s-worker02.linux.io 192.168.122.23 k8s-worker03 k8s-worker03.linux.io 192.168.122.11 k8s.linux.io EOF","title":"2.1 \u4e3b\u673a\u540d\u89e3\u6790"},{"location":"k8s/install_docker_k8s.html#22-docker","text":"\u914d\u7f6e apt \u6e90 # step 1: \u5b89\u88c5\u5fc5\u8981\u7684\u4e00\u4e9b\u7cfb\u7edf\u5de5\u5177 sudo apt-get update sudo apt-get install ca-certificates curl gnupg # step 2: \u4fe1\u4efb Docker \u7684 GPG \u516c\u94a5 sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Step 3: \u5199\u5165\u8f6f\u4ef6\u6e90\u4fe1\u606f echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null \u5b89\u88c5docker sudo apt-get update sudo apt-cache madison docker-ce sudo apt install -y docker-ce=5:20.10.22~3-0~ubuntu-focal \u914d\u7f6edocker\u52a0\u901f\u5668 mkdir -pv /etc/docker sudo cat > /etc/docker/daemon.json <<-'EOF' { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [ \"https://docker.rainbond.cc\" ] } EOF systemctl restart docker","title":"2.2 \u5b89\u88c5DOCKER"},{"location":"k8s/install_docker_k8s.html#23-cri-dockerd","text":"wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.8/cri-dockerd-0.3.8.amd64.tgz tar -xzvf cri-dockerd-0.3.8.amd64.tgz sudo install -m 0755 -o root -g root -t /usr/local/bin cri-dockerd/cri-dockerd wget https://raw.githubusercontent.com/Mirantis/cri-dockerd/master/packaging/systemd/cri-docker.service wget https://raw.githubusercontent.com/Mirantis/cri-dockerd/master/packaging/systemd/cri-docker.socket sudo install cri-docker.service /etc/systemd/system sudo install cri-docker.socket /etc/systemd/system sudo sed -i -e 's@/usr/bin/cri-dockerd@/usr/local/bin/cri-dockerd@' /etc/systemd/system/cri-docker.service sudo systemctl daemon-reload sudo systemctl enable --now cri-docker.socket sudo systemctl start cri-docker.service && systemctl status cri-docker.service","title":"2.3 \u5b89\u88c5 cri-dockerd"},{"location":"k8s/install_docker_k8s.html#24-kubeadmkubelet-kubectl","text":"\u914d\u7f6ekubernetes\u6e90 apt-get update && apt-get install -y apt-transport-https sudo mkdir -m 755 /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/ /\" | tee /etc/apt/sources.list.d/kubernetes.list apt-get update apt-cache madison kubeadm apt install -y kubeadm=1.28.6-1.1 kubelet=1.28.6-1.1 kubectl=1.28.6-1.1","title":"2.4 \u5b89\u88c5 kubeadm\u3001kubelet \u548c kubectl"},{"location":"k8s/install_docker_k8s.html#_3","text":"","title":"\u4e09\u3001\u521d\u59cb\u5316\u96c6\u7fa4"},{"location":"k8s/install_docker_k8s.html#31","text":"~]# kubeadm config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\ --kubernetes-version=1.28.6 \\ --cri-socket=unix:///var/run/cri-dockerd.sock [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.28.6 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.10-0 [config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.10.1","title":"3.1 \u62c9\u53d6\u955c\u50cf"},{"location":"k8s/install_docker_k8s.html#32-master","text":"\u914d\u7f6ecri-docker\u4e2d\u521d\u59cb\u5316\u5bb9\u5668 ~# cp /etc/systemd/system/cri-docker.service{,.bak} ~# vim /etc/systemd/system/cri-docker.service [Unit] ... [Service] Type=notify #ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// ExecStart=/usr/local/bin/cri-dockerd --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 --container-runtime-endpoint fd:// ~# systemctl daemon-reload && systemctl restart cri-docker.service && systemctl status cri-docker.service for i in k8s-worker01 k8s-worker02 k8s-worker03;do scp /etc/systemd/system/cri-docker.service root@$i:/etc/systemd/system/cri-docker.service ssh $i \"systemctl daemon-reload && systemctl restart cri-docker.service && systemctl status cri-docker.service\" done \u521d\u59cb\u5316master # \u6240\u6709\u8282\u70b9\u9700\u8981\u89e3\u6790 k8s.linux.io, \u6b64\u57df\u540d\u7528\u4e8e\u540e\u7eed\u6269\u5c55\u96c6\u7fa4\u4e3a\u9ad8\u53ef\u7528\u96c6\u7fa4 ~]# kubeadm init --kubernetes-version=v1.28.6 \\ --control-plane-endpoint=k8s.linux.io \\ --apiserver-advertise-address=0.0.0.0 \\ --pod-network-cidr=10.244.0.0/16 \\ --service-cidr=10.96.0.0/12 \\ --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\ --ignore-preflight-errors=Swap \\ --cri-socket=unix:///var/run/cri-dockerd.sock | tee kubeadm-init.log ... Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join k8s.linux.io:6443 --token 40zawh.qty5miole7ny5hf8 \\ --discovery-token-ca-cert-hash sha256:832dd459e1bf101d54f6ff80bec406f468aa9cd4b359c9536c845d696fdc8f21 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join k8s.linux.io:6443 --token 40zawh.qty5miole7ny5hf8 \\ --discovery-token-ca-cert-hash sha256:832dd459e1bf101d54f6ff80bec406f468aa9cd4b359c9536c845d696fdc8f21 \u914d\u7f6ekubectl ~# mkdir -p $HOME/.kube ~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config ~# sudo chown $(id -u):$(id -g) $HOME/.kube/config ~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 NotReady control-plane 113s v1.28.6","title":"3.2 \u521d\u59cb\u5316 master"},{"location":"k8s/install_docker_k8s.html#32-worker","text":"kubeadm join k8s.linux.io:6443 --token 40zawh.qty5miole7ny5hf8 \\ --discovery-token-ca-cert-hash sha256:832dd459e1bf101d54f6ff80bec406f468aa9cd4b359c9536c845d696fdc8f21 \\ --cri-socket=unix:///var/run/cri-dockerd.sock ~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 NotReady control-plane 9m28s v1.28.6 k8s-worker01 NotReady <none> 2m33s v1.28.6 k8s-worker02 NotReady <none> 2m30s v1.28.6 k8s-worker03 NotReady <none> 2m27s v1.28.6 ~# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6554b8b87f-k9jxl 0/1 Pending 0 9m17s kube-system coredns-6554b8b87f-tfkv8 0/1 Pending 0 9m17s kube-system etcd-k8s-master01 1/1 Running 0 9m29s kube-system kube-apiserver-k8s-master01 1/1 Running 0 9m29s kube-system kube-controller-manager-k8s-master01 1/1 Running 0 9m29s kube-system kube-proxy-fk5jk 1/1 Running 0 2m40s kube-system kube-proxy-hzqdp 1/1 Running 0 9m17s kube-system kube-proxy-l8grw 1/1 Running 0 2m43s kube-system kube-proxy-vgn8b 1/1 Running 0 2m37s kube-system kube-scheduler-k8s-master01 1/1 Running 0 9m29s","title":"3.2 \u52a0\u5165 Worker"},{"location":"k8s/install_docker_k8s.html#33","text":"\u5b89\u88c5 ~# wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml ~# grep -C 5 244 kube-flannel.yml } ] } net-conf.json: | { \"Network\": \"10.244.0.0/16\", \"EnableNFTables\": false, \"Backend\": { \"Type\": \"vxlan\" } } ~# grep image kube-flannel.yml image: docker.io/flannel/flannel:v0.26.1 image: docker.io/flannel/flannel-cni-plugin:v1.5.1-flannel2 image: docker.io/flannel/flannel:v0.26.1 ~# kubectl apply -f kube-flannel.yml ~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane 15m v1.28.6 k8s-worker01 Ready <none> 8m40s v1.28.6 k8s-worker02 Ready <none> 8m37s v1.28.6 k8s-worker03 Ready <none> 8m34s v1.28.6 ~# kubectl get pod -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-flannel kube-flannel-ds-h8zrm 1/1 Running 0 102s 192.168.122.21 k8s-worker01 <none> <none> kube-flannel kube-flannel-ds-l8cgv 1/1 Running 0 102s 192.168.122.23 k8s-worker03 <none> <none> kube-flannel kube-flannel-ds-q2nk2 1/1 Running 0 102s 192.168.122.11 k8s-master01 <none> <none> kube-flannel kube-flannel-ds-tg7r9 1/1 Running 0 102s 192.168.122.22 k8s-worker02 <none> <none> kube-system coredns-6554b8b87f-k9jxl 1/1 Running 0 15m 10.244.0.3 k8s-master01 <none> <none> kube-system coredns-6554b8b87f-tfkv8 1/1 Running 0 15m 10.244.0.2 k8s-master01 <none> <none> kube-system etcd-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-apiserver-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-controller-manager-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-proxy-fk5jk 1/1 Running 0 8m45s 192.168.122.22 k8s-worker02 <none> <none> kube-system kube-proxy-hzqdp 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> kube-system kube-proxy-l8grw 1/1 Running 0 8m48s 192.168.122.21 k8s-worker01 <none> <none> kube-system kube-proxy-vgn8b 1/1 Running 0 8m42s 192.168.122.23 k8s-worker03 <none> <none> kube-system kube-scheduler-k8s-master01 1/1 Running 0 15m 192.168.122.11 k8s-master01 <none> <none> \u8c03\u6574\u4e3aipvs\u6a21\u5f0f kubectl edit cm kube-proxy -n kube-system mode: \"ipvs\" kubectl delete pod -n kube-system -l k8s-app=kube-proxy","title":"3.3 \u5b89\u88c5\u7f51\u7edc\u63d2\u4ef6"},{"location":"k8s/install_docker_k8s.html#_4","text":"","title":"\u56db\u3001\u9a8c\u8bc1\u96c6\u7fa4"},{"location":"k8s/install_docker_k8s.html#41","text":"\u521b\u5efa Deployment \u548c Service \u8d44\u6e90 ~# kubectl create deployment myapp --image=ikubernetes/myapp:v1 --replicas=3 ~# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myapp-5d9c4b4647-7xmkx 1/1 Running 0 31s 10.244.2.2 k8s-worker02 <none> <none> myapp-5d9c4b4647-qrkdl 1/1 Running 0 31s 10.244.1.2 k8s-worker01 <none> <none> myapp-5d9c4b4647-w9lq7 1/1 Running 0 31s 10.244.3.2 k8s-worker03 <none> <none> ~# kubectl expose deployment/myapp --type=NodePort --port=80 --target-port=80 ~# kubectl get svc myapp -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR myapp NodePort 10.106.198.16 <none> 80:30940/TCP 25s app=myapp \u901a\u8fc7 Service \u6216\u8005 NodePort \u8d44\u6e90\u8f6e\u8be2 Pod ~# for i in `seq 5`;do curl 10.106.198.16/hostname.html;done myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-7xmkx ~]# for i in `seq 5`; do curl 192.168.122.11:30940/hostname.html;done myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-w9lq7 myapp-5d9c4b4647-qrkdl myapp-5d9c4b4647-7xmkx \u6ce8\u610f\uff1aService IP \u548c NodePort \u4ec5\u662f\u7f51\u7edc\u89c4\u5219\uff0c\u6240\u4ee5\u4e0d\u652f\u6301ping\uff0c \u4f46\u662f\u652f\u6301telnet","title":"4.1 \u9a8c\u8bc1\u7f51\u7edc"},{"location":"k8s/install_docker_k8s.html#41-dns","text":"~# kubectl get svc -n kube-system -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kube-dns ClusterIP 10.96.0.10 <none> 53/UDP,53/TCP,9153/TCP 29m k8s-app=kube-dns ~# kubectl run busybox --image=busybox:1.28 -- sleep 3600 ~# kubectl get pod/busybox NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 16s ~# kubectl exec -it busybox -- nslookup myapp.default.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: myapp.default.svc.cluster.local Address 1: 10.106.198.16 myapp.default.svc.cluster.local","title":"4.1 \u9a8c\u8bc1DNS"},{"location":"k8s/install_docker_k8s.html#_5","text":"","title":"\u4e94\u3001\u63d2\u4ef6\u5b89\u88c5"},{"location":"k8s/install_docker_k8s.html#51-metrics-server","text":"\u5b98\u7f51\uff1a https://github.com/kubernetes-sigs/metrics-server ~# wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml ~# vim components.yaml spec: containers: - args: - --cert-dir=/tmp - --secure-port=10250 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls ~# kubectl apply -f components.yaml ~# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 189m 4% 1766Mi 22% k8s-worker01 61m 1% 964Mi 12% k8s-worker02 59m 1% 913Mi 11% k8s-worker03 61m 1% 937Mi 12% ~# kubectl top pods NAME CPU(cores) MEMORY(bytes) busybox 0m 1Mi myapp-5d9c4b4647-4t4sp 0m 2Mi myapp-5d9c4b4647-fb4kh 0m 2Mi myapp-5d9c4b4647-zgqtw 0m 2Mi","title":"5.1 Metrics-Server"},{"location":"k8s/install_docker_k8s.html#52-dashboard","text":"\u5b98\u7f51\uff1a https://github.com/kubernetes/dashboard \u90e8\u7f72 dashboard wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml kubectl apply -f recommended.yaml kubectl patch -n kubernetes-dashboard svc/kubernetes-dashboard -p '{\"spec\": {\"type\": \"NodePort\"}}' # kubectl get svc -n kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.104.44.78 <none> 8000/TCP 17m kubernetes-dashboard NodePort 10.104.155.213 <none> 443:30106/TCP 17m \u521b\u5efa\u7528\u6237 cat admin-user.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard kubectl apply -f admin-user.yaml kubectl -n kubernetes-dashboard create token admin-user \u8bbf\u95ee \u5982\u679cgoogle\u4e0d\u5141\u8bb8\u8bbf\u95eehttps\uff0c\u8bf7\u5728\u8bbf\u95ee\u9875\u9762\u8f93\u5165 thisisunsafe \u89e3\u51b3","title":"5.2 Dashboard"},{"location":"python/advance.html","text":"Python\u8fdb\u9636 \u00b6 \u6570\u636e\u7ed3\u6784 \u00b6","title":"Python\u8fdb\u9636"},{"location":"python/advance.html#python","text":"","title":"Python\u8fdb\u9636"},{"location":"python/advance.html#_1","text":"","title":"\u6570\u636e\u7ed3\u6784"}]}